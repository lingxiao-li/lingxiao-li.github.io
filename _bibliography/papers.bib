---
---
@String(PAMI = {IEEE Trans. Pattern Anal. Mach. Intell.})
@String(IJCV = {Int. J. Comput. Vis.})
@String(CVPR= {IEEE Conf. Comput. Vis. Pattern Recog.})
@String(ICCV= {Int. Conf. Comput. Vis.})
@String(ECCV= {Eur. Conf. Comput. Vis.})
@String(NIPS= {Adv. Neural Inform. Process. Syst.})
@String(ICPR = {Int. Conf. Pattern Recog.})
@String(BMVC= {Brit. Mach. Vis. Conf.})
@String(TOG= {ACM Trans. Graph.})
@String(TIP  = {IEEE Trans. Image Process.})
@String(TVCG  = {IEEE Trans. Vis. Comput. Graph.})
@String(TMM  = {IEEE Trans. Multimedia})
@String(ACMMM= {ACM Int. Conf. Multimedia})
@String(ICME = {Int. Conf. Multimedia and Expo})
@String(ICASSP=	{ICASSP})
@String(ICIP = {IEEE Int. Conf. Image Process.})
@String(ACCV  = {ACCV})
@String(ICLR = {Int. Conf. Learn. Represent.})
@String(IJCAI = {IJCAI})
@String(PR   = {Pattern Recognition})
@String(AAAI = {AAAI})
@String(CVPRW= {IEEE Conf. Comput. Vis. Pattern Recog. Worksh.})
@String(CSVT = {IEEE Trans. Circuit Syst. Video Technol.})

@String(SPL	= {IEEE Sign. Process. Letters})
@String(VR   = {Vis. Res.})
@String(JOV	 = {J. Vis.})
@String(TVC  = {The Vis. Comput.})
@String(JCST  = {J. Comput. Sci. Tech.})
@String(CGF  = {Comput. Graph. Forum})
@String(CVM = {Computational Visual Media})


@String(PAMI  = {IEEE TPAMI})
@String(IJCV  = {IJCV})
@String(CVPR  = {CVPR})
@String(ICCV  = {ICCV})
@String(ECCV  = {ECCV})
@String(NIPS  = {NeurIPS})
@String(ICPR  = {ICPR})
@String(BMVC  =	{BMVC})
@String(TOG   = {ACM TOG})
@String(TIP   = {IEEE TIP})
@String(TVCG  = {IEEE TVCG})
@String(TCSVT = {IEEE TCSVT})
@String(TMM   =	{IEEE TMM})
@String(ACMMM = {ACM MM})
@String(ICME  =	{ICME})
@String(ICASSP=	{ICASSP})
@String(ICIP  = {ICIP})
@String(ACCV  = {ACCV})
@String(ICLR  = {ICLR})
@String(IJCAI = {IJCAI})
@String(PR = {PR})
@String(AAAI = {AAAI})
@String(CVPRW= {CVPRW})
@String(CSVT = {IEEE TCSVT})

@INPROCEEDINGS{Li25HypDAE,
title	= {HypDAE: Hyperbolic Diffusion Autoencoders for Hierarchical Few-shot Image Generation},
abbr = {ICCV},
author	= {Lingxiao Li and Kaixuan Fan and Boqing Gong and Xiangyu Yue},
year	= {2025},
abstract = {Few-shot image generation aims to generate diverse and high-quality images for an unseen class given only a few examples in that class. However, existing methods often suffer from a trade-off between image quality and diversity while offering limited control over the attributes of newly generated images. In this work, we propose Hyperbolic Diffusion Autoencoders (HypDAE), a novel approach that operates in hyperbolic space to capture hierarchical relationships among images and texts from seen categories. By leveraging pre-trained foundation models, HypDAE generates diverse new images for unseen categories with exceptional quality by varying semantic codes or guided by textual instructions. Most importantly, the hyperbolic representation introduces an additional degree of control over semantic diversity through the adjustment of radii within the hyperbolic disk. Extensive experiments and visualizations demonstrate that HypDAE significantly outperforms prior methods by achieving a superior balance between quality and diversity with limited data and offers a highly controllable and interpretable generation process.},
booktitle	= {International Conference on Computer Vision (ICCV)},
selected = {true},
pdf = {https://arxiv.org/abs/2411.17784}
}

@INPROCEEDINGS{Hao25,
title	= {Nazar: Monitoring and Adapting ML Models on Mobile Devices},
abbr={ASPLOS},
author	= {Wei Hao and Zixi Wang and Lauren Hong and Lingxiao Li and Nader Karayanni and Chengzhi Mao and Junfeng Yang and Asaf Cidon},
year	= {2025},
abstract = {ML models are increasingly being pushed to mobile devices, for low-latency inference and offline operation. However, once the models are deployed, it is hard for ML operators to track their accuracy, which can degrade unpredictably (e.g., due to data drift). We design Nazar, the first end-to-end system for continuously monitoring and adapting models on mobile devices without requiring feedback from users. Our key observation is that often model degradation is due to a specific root cause, which may affect a large group of devices. Therefore, once Nazar detects a consistent degradation across a large number of devices, it employs a root cause analysis to determine the origin of the problem and applies a cause-specific adaptation. We evaluate Nazar on two computer vision datasets, and show it consistently boosts accuracy compared to existing approaches. On a dataset containing photos collected from driving cars, Nazar improves the accuracy on average by 15%.},
booktitle	= {ACM International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS)},
selected = {true},
//honor = {Oral Presentation [top 2.5%]},
//award = {Oral},
pdf = {https://arxiv.org/abs/2305.07772}
}


@ARTICLE{Zhang25Radar,
author={Zhang, Yuanyuan and Guan, Runwei and Li, Lingxiao and Yang, Rui and Yue, Yutao and Lim, Eng Gee},
journal={ IEEE Transactions on Mobile Computing },
title={ radarODE: an ODE-Embedded Deep Learning Model for Contactless ECG Reconstruction from Millimeter-Wave Radar },
abbr={TMC},
year={2025},
abstract={ Radar-based cardiac monitoring has become a popular research direction recently, but the fine-grained electrocardiogram (ECG) signal is still hard to reconstruct from millimeter-wave radar signal. The key obstacle is to decouple cardiac activities in the electrical domain (i.e., ECG) from that in the mechanical domain (i.e., heartbeat), and most existing research only uses purely data-driven methods to map such domain transformation as a black box. Therefore, this work first proposes a signal model that considers the fine-grained cardiac feature sensed by radar, and a novel deep learning framework called radarODE is designed to extract both temporal and morphological features for generating ECG. In addition, ordinary differential equations are embedded in radarODE as a decoder to provide morphological prior, helping the convergence of the model training and improving the robustness under body movements. After being validated on the dataset, the proposed radarODE achieves better performance compared with the benchmark in terms of missed detection rate, root mean square error, Pearson correlation coefficient with improvements of 9%, 16% and 19%, respectively. The validation results imply that radarODE is capable of recovering ECG signals from radar signals with high fidelity and can potentially be implemented in real-life scenarios },
url = {https://doi.ieeecomputersociety.org/10.1109/TMC.2025.3563945},
publisher={IEEE Computer Society},
address={Los Alamitos, CA, USA},
selected = {true},
pdf={https://www.computer.org/csdl/journal/tm/5555/01/10975137/269aO6bb2k8}
}

@INPROCEEDINGS{Li24,
title	= {BIFRÖST: 3D-Aware Image compositing with Language Instructions},
abbr={NeurIPS},
author	= {Lingxiao Li and Kaixiong Gong and Weihong Li and Xili Dai and Tao Chen and Xiaojun Yuan and Xiangyu Yue},
year	= {2024},
abstract = {This paper introduces Bifröst, a novel 3D-aware framework that is built upon diffusion models to perform instruction-based image composition. Previous methods concentrate on image compositing at the 2D level, which fall short in handling complex spatial relationships (e.g., occlusion). Bifröst addresses these issues by training MLLM as a 2.5D location predictor and integrating depth maps as an extra condition during the generation process to bridge the gap between 2D and 3D, which enhances spatial comprehension and supports sophisticated spatial interactions. Our method begins by fine-tuning MLLM with a custom counterfactual dataset to predict 2.5D object locations in complex backgrounds from language instructions. Then, the image-compositing model is uniquely designed to process multiple types of input features, enabling it to perform high-fidelity image compositions that consider occlusion, depth blur, and image harmonization. Extensive qualitative and quantitative evaluations demonstrate that Bifröst significantly outperforms existing methods, providing a robust solution for generating realistically composed images in scenarios demanding intricate spatial understanding. This work not only pushes the boundaries of generative image compositing but also reduces reliance on expensive annotated datasets by effectively utilizing existing resources in innovative ways.},
booktitle	= {Advanced Neural Information Processing System (NeurIPS)},
selected = {true},
//honor = {Oral Presentation [top 2.5%]},
//award = {Oral},
pdf = {https://arxiv.org/abs/2410.19079}
}


@INPROCEEDINGS{Li23,
title	= {The Euclidean Space is Evil: Hyperbolic Attribute Editing for Few-shot Image Generation},
abbr = {ICCV},
author	= {Lingxiao Li and Yi Zhang and Shuhui Wang},
year	= {2023},
abstract = {Few-shot image generation is a challenging task since it aims to generate diverse new images for an unseen category with only a few images. Existing methods suffer from the trade-off between the quality and diversity of generated images. To tackle this problem, we propose Hyperbolic Attribute Editing(HAE), a simple yet effective method. Unlike prior arts that work in Euclidean space, HAE captures the hierarchy among images using data from seen categories in hyperbolic space. Given a well-trained HAE, images of unseen categories can be generated by moving the latent code of a given image toward any  meaningful directions in the Poincaré disk with a fixing radius. Most importantly, the hyperbolic space allows us to control the semantic diversity of the generated images by setting different radii in the disk. Extensive experiments and visualizations demonstrate that HAE is capable of not only generating images with promising quality and diversity using limited data but achieving a highly controllable and interpretable editing process.},
booktitle	= {International Conference on Computer Vision (ICCV)},
selected = {true},
//honor = {Oral Presentation [top 2.5%]},
//award = {Oral},
pdf = {https://openaccess.thecvf.com/content/ICCV2023/html/Li_The_Euclidean_Space_is_Evil_Hyperbolic_Attribute_Editing_for_Few-shot_ICCV_2023_paper.html}
}

@INPROCEEDINGS{Ni18,
  author={Ni, Congwei and Cheng, Sihan and Wang, Xutao and Hu, Tianyun and Dai, Zhenjin and Zhang, Dongliang and Li, Lingxiao and Huang, Xin},
  abbr={ITME},
  booktitle={9th International Conference on Information Technology in Medicine and Education (ITME)}, 
  title={Model Checking the Reliability of Data Center Network}, 
  abstract={Data center network provides a basis for internet-based applications nowadays, thereby the reliability of it is vital. The purpose of this research is to examine and compare the reliability of two data center network architectures, which are Common Tree and Fat-tree topology structure. To verify the reliability of these two architectures, a tool called PRISM with the function of probabilistic model checking is used in this paper. Results show that Fat-tree topology structure is more reliable than Common Tree, and the reliabilities of core layer, edge/ aggregation layer and terminal layer decline sequentially.},
  year={2018},
  pages={882-884},
  doi={10.1109/ITME.2018.00197},
  pdf = {https://ieeexplore.ieee.org/abstract/document/8589429},
  url={https://ieeexplore.ieee.org/abstract/document/8589429}}

